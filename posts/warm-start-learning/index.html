<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Warm-start Learning | Steven's Blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Let&rsquo;s say you&rsquo;re training a text classification system, for example, to automatically classify support tickets into buckets, &ldquo;urgent&rdquo; and &ldquo;not urgent&rdquo;. You have historical data on the support tickets and some labels on whether or not it was urgent.
The goal is to develop a useful predictive model to help triage these tickets.
Below, we describe a handful of techniques that are worth considering when building a binary model.
The main definition we will have is warm-start learning, as this is the category of techniques I will describe."><meta name=generator content="Hugo 0.95.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><meta property="og:title" content="Warm-start Learning"><meta property="og:description" content="Let&rsquo;s say you&rsquo;re training a text classification system, for example, to automatically classify support tickets into buckets, &ldquo;urgent&rdquo; and &ldquo;not urgent&rdquo;. You have historical data on the support tickets and some labels on whether or not it was urgent.
The goal is to develop a useful predictive model to help triage these tickets.
Below, we describe a handful of techniques that are worth considering when building a binary model.
The main definition we will have is warm-start learning, as this is the category of techniques I will describe."><meta property="og:type" content="article"><meta property="og:url" content="https://blog.tartakovsky.org/posts/warm-start-learning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-22T22:54:35-07:00"><meta property="article:modified_time" content="2022-03-22T22:54:35-07:00"><meta itemprop=name content="Warm-start Learning"><meta itemprop=description content="Let&rsquo;s say you&rsquo;re training a text classification system, for example, to automatically classify support tickets into buckets, &ldquo;urgent&rdquo; and &ldquo;not urgent&rdquo;. You have historical data on the support tickets and some labels on whether or not it was urgent.
The goal is to develop a useful predictive model to help triage these tickets.
Below, we describe a handful of techniques that are worth considering when building a binary model.
The main definition we will have is warm-start learning, as this is the category of techniques I will describe."><meta itemprop=datePublished content="2022-03-22T22:54:35-07:00"><meta itemprop=dateModified content="2022-03-22T22:54:35-07:00"><meta itemprop=wordCount content="376"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Warm-start Learning"><meta name=twitter:description content="Let&rsquo;s say you&rsquo;re training a text classification system, for example, to automatically classify support tickets into buckets, &ldquo;urgent&rdquo; and &ldquo;not urgent&rdquo;. You have historical data on the support tickets and some labels on whether or not it was urgent.
The goal is to develop a useful predictive model to help triage these tickets.
Below, we describe a handful of techniques that are worth considering when building a binary model.
The main definition we will have is warm-start learning, as this is the category of techniques I will describe."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Steven's Blog</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Warm-start Learning</h1><time class="f6 mv4 dib tracked" datetime=2022-03-22T22:54:35-07:00>March 22, 2022</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Let&rsquo;s say you&rsquo;re training a text classification system, for example, to automatically classify support tickets into buckets, &ldquo;urgent&rdquo; and &ldquo;not urgent&rdquo;. You have historical data on the support tickets and some labels on whether or not it was urgent.</p><p>The goal is to develop a useful predictive model to help triage these tickets.</p><p>Below, we describe a handful of techniques that are worth considering when building a binary model.</p><p>The main definition we will have is <em>warm-start</em> learning, as this is the category of techniques I will describe. These techniques are characterized by using a pretrained neural network language model, such as <a href=https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2>this one</a> from Microsoft.</p><ol><li><p><strong>Full fine tuning</strong> Freeze none of the weights and simply run your speciic data through to, exactly like training a neural network on a binary classification task (eg. Use cross entropy loss, etc.). Read more about fine tuning <a href=https://huggingface.co/docs/transformers/training>here</a>.</p></li><li><p><strong>Last layer tuning</strong> Freeze all of the weights except the second-to-last last layer, and do the same as the previous step. This is like running logistic regression on the embeddings produced on the pretrained model.</p></li><li><p><strong>Transfer learning</strong> A more general case of the previous step is Transfer learning. This time, use the embeddings and apply any classifer. So take it out of the neural net and use any classifier, not just Logistic Regresssion.</p></li><li><p><strong>Nearest neighbor search</strong> Here don&rsquo;t run any training, 1) embed the result and 2) be ready to determine whether the majority of top-k, or top-k% nearest neighbors are &ldquo;urgent&rdquo;; if so, label them urgent. k=1 may be a good starting point. Or, time permitting, a validation procedure can be done to select k.</p></li></ol><p>I assume the thesis of HuggingFace was something like:</p><p>Model training is hard and takes lots of infrastructure. Infrastructure is costly and push button neural network training is not yet democratized with models of scale. Models are large with - in a growning number of cases and domains beyond NLP, billions of parameters.</p><p>For most of us data scientists and machine learners, the days training of models from scratch (i.e. determining the &ldquo;best&rdquo; way to randomly initialize parameters), are over. Noadays, warm-start learning is what we should expect with complex data. It&rsquo;s now a question of which warm-start technique to employ at a given use-case.</p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://blog.tartakovsky.org/>&copy; Steven's Blog 2022</a><div><div class=ananke-socials></div></div></div></footer></body></html>